\documentclass[ % ドキュメントクラス
  uplatex,%upLaTeXを使う
  papersize%紙のサイズがデフォルトと違う場合、PDFにうまく伝える
]{jsarticle}

%% フォント関連
\usepackage{authblk}
\usepackage{diagbox}
\usepackage{bm}
\usepackage[T1]{fontenc} % フォントでT1を使うこと
\usepackage{textcomp} % フォントでTS1を使うこと
\usepackage[utf8]{inputenc} % ファイルがUTF8であること
\usepackage{newpxtext,newpxmath} % ローマンと数式の字体をPalatinoに基づいた新PXフォントで
\usepackage{zi4}%等幅フォントをInconsolataで
\usepackage[multi,deluxe,jis2004]{otf}
\usepackage[prefernoncjk]{pxcjkcat} % なるべく「半角」扱いで。
\cjkcategory{sym18}{cjk} % sym18 (U+25A0 - U+25FF Geometric Shapes) を和文文字あつかい

%% 図表など
% 図の読みこみのために
\usepackage[dvipdfmx, hiresbb]{graphicx, xcolor}

\usepackage{multirow}
\usepackage{multicol}

\usepackage{booktabs} % 表の横罫線
\usepackage{lscape}  % 表などを90度回転させる

%% 囲み枠
\usepackage{tcolorbox}
\tcbuselibrary{breakable} % ページをまたいで分割できるように

% ソースコードをきれいに出力する
\usepackage{listings}
\lstset{ % 色々な設定
    basicstyle=\ttfamily\scriptsize, % 基本的にタイプライター体にする
    %numbers=left, % 左側に行番号を表示
    %numberstyle=\small, % 行番号は小さめに
    %numbersep=16pt, % 行番号をどれだけ離すか
    % showspaces=true,% スペースを表示したければtrueにする 
    xleftmargin=15pt, % 左側のマージン
    frame=single, %ソースコードを囲むフレーム
    framesep=10pt, %フレームとコードの間隔
    backgroundcolor=\color[gray]{0.95}, % 背景色
    breaklines=true, % 長い行は改行する
    commentstyle=\color{red},
    keywordstyle=\color{blue}
}
\renewcommand{\lstlistingname}{ソースコード}

% misc
\usepackage{okumacro} % 圏点などのため
\usepackage{pxrubrica} % ルビをつける（okumacroのrubyは使わない）

% hyperrefの設定
\usepackage[dvipdfmx,%
   bookmarks=true,%PDFにしおりをつける
   bookmarksnumbered=true,%しおりに節番号などをつける
   colorlinks=false,%リンクには色をつけない
   hyperfootnotes=false,%脚注からのリンクを作らない
   pdfborder={0 0 0},%枠なし
   pdfpagemode=UseNone]{hyperref}
   

% PDFにしたときの文字化けを防ぐ
\usepackage{pxjahyper}

\title{パターン認識 -- 課題3}

\author{\large GENG Haopeng 611710008 \\ \small Email:  \href{mailto:kevingenghaopeng@gmail.com}{kevingenghaopeng@gmail.com}}
\affil{\small Department of Intelligent Systems, Nagoya University}

\date{\today}

\begin{document}
\maketitle

\begin{abstract}
今回の課題は、パーセプトロンによる学習の基礎および応用である。

さらに、3種の認識方法を理解した上、プログラムを作成し、実行結果による評価と考察を行う。

そのほか、課題のソースコードが既に\href{https://github.com/Secondtonumb/pattern_recogn/tree/master/pattern03}{Github Repository}に掲載されるため、参照あるいは実行することは可能である。
\end{abstract}


\section{パーセプトロンによる学習(基本編)}
\subsection{実験理論およびアルゴリズム}
この課題は、パーセプトロンの収束定理を利用し、重みベクトルを修正しつつ、線形分離可能なパターンであれば、解領域内の重みベクトルに到達する。いわゆる、全ての学習パターンが正しく識別される。
アルゴリズムの手順は以下のように表す：
\begin{itemize}
\small
\item[1] 重みベクトル$\bm{w}$の初期値を決める
\item[2] パターン集合$\mathcal{X}$から順番で一つのパターン$\omega_{i}$を選ぶ
\item[3] 識別関数
	$$g(\bm{x})=\bm{w}^{t}\bm{x}$$
	によって識別を行い。正しく識別できなかった場合、次のように重みベクトルを修正する。
	$$\bm{w}^{'}= \bm{w} +\rho\bm{x}    (\omega_{i}\in\mathcal{X},g_{i}(\bm{x})\neq \max\{g_{1}(\bm{x}),g_{2}(\bm{x}),\cdots,g_{n}(\bm{x}) \})$$
	$$\bm{w}^{'}= \bm{w} -\rho\bm{x}     (\omega_{j}\in\mathcal{X},g_{j}(\bm{x})= \max\{g_{1}(\bm{x}),g_{2}(\bm{x}),\cdots,g_{n}(\bm{x}) \})$$
\item[4] 上の処理[2], [3]全てのパターンに対して繰り返す
\item[5]  $\forall \omega_{i}\in\mathcal{X}$,正しく識別できたら終了、誤りがあるときは[2]に戻る
\end{itemize}

ただし、最大値が複数個出現する場合、計算順で最初に出た重みのみ修正する。

\subsection{プログラムに工夫した部分}
誤り識別するとき、重みベクトルの修正は以下のように
\begin{lstlisting}[language=c,caption=correct weight vector,]
 /* calculate until converge, while converge condition is all pattern have become stable */
  while(conv(flag, LEARNING_NUM)){

    if(p >= LEARNING_NUM) p = p % LEARNING_NUM;

    for(j = 0; j < CLU_NUM; j++){
      eval[j] = multi(p_arr[p].data, w[j], CLU_NUM);
      printf("Evaluation of Pattern %d : %f\n", p, eval[j]);
    }

    /* Corr is pattern's true Class */
    /* Err is Evaluated Class, if condition is met , err == -1 */
    int corr = p_arr[p].pclass;
    int err = judge_max(eval, CLU_NUM, corr);

    printf("Right Class = %d,Error Class = %d\n",corr,err);

    if(err != -1){
      flag[p] = 1;
      for(j = 0; j < W_DIM; j++){
        w[corr][j] += rho * p_arr[p].data[j];
        w[err][j] -= rho * p_arr[p].data[j];
      }
    }

    /* Weight Condition is met, change flag to let result converge */
    else{
      flag[p] = 0;
    }

    printf("========\n");

    for(i = 0; i < CLU_NUM; i++){
      for(j = 0; j < W_DIM; j++){
        printf("%.0f ",w[i][j]);
      }
      printf("\n");
    }
    p++;
  }
\end{lstlisting}

\subsection{プログラム実行例}
\subsubsection{重みベクトルの修正}
与えられたパターンと初期重みを用い、以下のように出力される。ただし、20回の繰り返し演算を略し、結果のみを表す。
\begin{lstlisting}[language=bash]
$./pl learning_data.list init_weight.dat weight.dat
.
.
.
Evaluation of Pattern 1 by g(0) : 13.000000
Evaluation of Pattern 1 by g(1) : 9.000000
Evaluation of Pattern 1 by g(2) : 12.000000
Right Class = 0,Error Class = -1
========
9 2 0
2 1 5
-2 6 2
\end{lstlisting}

出力結果について、

\begin{itemize}
\item[1] 各評価関数の値
\item[2] 所属すべきクラス、識別関数による識別結果
\item[3] 修正された重みベクトル
\end{itemize}

という順番で示され、ゆえに、合計20回の修正で、学習結果である重みベクトルは
$
 \begin{bmatrix}
   9 & 2 & 0 \\
   2 & 1 & 5 \\
   -2 & 6 & 2
  \end{bmatrix}
$になった。

\subsubsection{未知パターン識別}
上記の重みベクトルを用い、未知パターンの識別を行った。
\begin{lstlisting}[language=bash]
$./pl_rec weight.dat unknown.dat
Value of g[0]: 13.000000
Value of g[1]: 14.000000
Value of g[2]: 14.000000
recog result == -1
\end{lstlisting}

ゆえに、識別関数の最大値が複数個（g[1], g[2]）あるため、識別できない結果になった。

\subsection{考察}
\subsubsection{初期ベクトルの変化による影響}
$\bm{W_{init}}$をランダムに500個を選び、識別結果を評価した結果は表１で示す。

\begin{table}[h]\small
\centering
\caption{500個の初期重みベクトルの実験結果}
\label{table}
\begin{tabular}{|l|c|c|c|c|} 
\hline  
識別結果 & ${w_{1}}$ & ${w_{2}}$ & ${w_{3}}$ & Unrec  \\
\hline
識別数 & 131 & 84 & 180 & 105 \\
\hline
比率 & 26.2\% & 16.8\% & 36.0\% & 21.0\%\\
\hline                       
\end{tabular} 
\end{table}

ただし、各重みベクトル$\bm{w_{i}}$の要素$w_{ij}$に対し,以下の条件を満たす。

$$w_{ij}\in[-5, 5] , w_{ij} \in \bm{Z}$$

よって、正しい識別率は26.2\%になった。

\subsubsection{定数$\rho$の変化による影響}
定数$\rho$を0.1から1.5まで、刻み幅を0.1、および$\rho = 2.0, 3.0,4.0,5.0,6.0$に設定し実験した結果は表2のように表す。
\begin{table}[h]\small
\centering
\caption{$\rho$を変化させてできた実験結果}
\label{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|} 
\hline  
$\rho$ & 識別結果 & $\rho$& 識別結果& $\rho$ & 識別結果& $\rho$ & 識別結果 &識別率\\
\hline
0.1 & ${\surd}$& 0.6 & ${\surd}$ & 1.1 & Err($w_{2}$)& 2.0& ${\surd}$&\multirow{5}*{75.0\%}\\
\cline{1-8}
0.2 & ${\surd}$& 0.7 & ${\surd}$ & 1.2 &  ${\surd}$ & 3.0& Unrec & \\
\cline{1-8}
0.3 & ${\surd}$& 0.8 & ${\surd}$& 1.3 &  ${\surd}$& 4.0& ${\surd}$ & \\
\cline{1-8}
0.4 & ${\surd}$& 0.9 & ${\surd}$& 1.4 &  ${\surd}$ & 5.0& ${\surd}$ &\\
\cline{1-8}
0.5 & Unrec & 1.0 & Unrec & 1.5 &  Unrec & 6.0& ${\surd}$ &\\
\hline                       
\end{tabular} 
\end{table}

実験結果により、$\rho = 0.5 ,1.0, 1.5, 3.0$の時、識別できない場合は多いため、初期重みとパターンの値は整数であるは原因であると考える。

\section{プロトタイプの最近傍決定則による文字認識}
\subsection{実験目的および理論}
この課題は、学習パターンからプロトタイプを生成する。そして、未知パターンを先用いた学習パターンではなく、各プロトタイプとの距離の計算し、一番近いプロトタイプは識別結果になる。
ただし、プロトタイプの生成方法に関して、今回の課題は、重心(各種パターンの平均値)とする。

$$\bm{P}=\frac{1}{n}\sum_{i=0}^{n}\bm{X_{i}}$$

ただし、生成されたプロトタイプは平均値はほとんど小数になるが、未知パターンの各次元とプロトタイプの各次元の絶対値の和とその絶対値の平方の和の大小関係は同じであることが分かった。即ち、Euclidean 距離を計算しなくとも正しい結果が得られることである。

下記の実行例と識別結果は、「距離」は各次元の差の絶対値の和として理解すれば良い。

\subsection{プログラムの解説}
\subsubsection{プログラムの流れ}

\begin{enumerate}
\small
\item 学習パターンをファイルから入力し、各パターンに応じて、プロトタープを生成し保存。
\item 未知パターンを入力し保存。
\item 各プロトタイプと未知パターンの距離を計算し、ソートして距離の最小値とそれに相応しいパターンをリターンする。
\item 戻り値は結果として認められる。
\end{enumerate}

\subsection{プログラム実行例}

pattern7-5.datを例として、以下のように出力される。

\begin{lstlisting}[language=bash]
$../proto_nnm proto.list unrecog_data/pattern7-5.dat
==> unrecog_data/pattern7-5.dat <==
1 1 1 1 1
1 1 0 0 1
0 0 0 1 0
0 0 1 0 0
0 1 1 0 0

==> Valuation module ==<
Distence with Prototype [2] : 6.800000
Distence with Prototype [7] : 5.600000
Distence with Prototype [9] : 11.000000

==> Result of PATTERN by Prototype Method <==
Recognition Result == 7
\end{lstlisting}

出力結果について、

\begin{itemize}
\item[1] 未知パターンの二値表示
\item[2] 各プロトタイプとの距離
\item[3] 識別結果
\end{itemize}
の順番で表す。ゆえに、pattern7-5 の識別結果は７であることが分かった。

\subsection{識別結果}
プロトタイプを用いる最近傍法を用いて、３種類の数字で各2文字ずつを認識した結果は表3のように表す。
\begin{table}[h]\small
\caption{プロトタイプを用いる最近傍法の識別結果}
\label{プロトタイプ}
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\multirow{2}{*}{\diagbox{未知パターン}{評価項目}} &
\multicolumn{3}{c|}{各プロトタイプとの距離} &
\multirow{2}{*}{識別結果(最近傍プロトタイプ)}\\
\cline{2-4}
  & Prototype2 & Prototype7 & Prototype9 &\\
\hline
pattern2-5 & 4.4 & 8.4 & 11.0 & prototype2 \\
pattern2-5 & 5.2 & 9.6 & 11.8 & prototype2 \\
pattern7-5 & 6.8 & 5.6 & 11.0 & prototype7 \\
pattern7-6 & 12.4 & 9.2 & 10.2 & prototype7 \\
pattern9-5 & 9.6 & 9.2 & 7.4 & prototype9 \\
pattern9-6 & 8.8 & 12.0 & 10.2 & prototype2 \\
\hline                                               
\end{tabular} 
\end{table}

表3によって、pattern9-6以外の場合は正しく識別される。識別率は$5\div6 \approx 83.3\%$である。

\section{考察}
\subsection{プログラムの頑丈性}
K近傍法に応じて、Kをいくつかの値を用い考察した結果は表4のように表す。
\begin{table}[h]\small
\centering
\caption{K近傍法の考察結果}
\label{table}
\begin{tabular}{|c|ccccc|c|}
\hline
\diagbox{未知パターン}{評価結果}{Kの値} & $K=1$ & $K=3$ &$K=5$ &$K=7$ &$K=9$ & 正確率\\
\hline
pattern2-5 & ${\surd}$& ${\surd}$ &${\surd}$ & ${\surd}$&${\surd}$&$100\%$ \\
pattern2-6 & ${\surd}$& ${\surd}$ &${\surd}$ & ${\surd}$ &${\surd}$&$100\%$ \\
pattern7-5 & ${\surd}$& ${\surd}$ &${\surd}$ & Err(2) &${\surd}$&$80\%$ \\
pattern7-5 & ${\surd}$& ${\surd}$ &Rej(7,9) & Rej(7,9) &Err(9)&$40\%$ \\
pattern9-5 & ${\surd}$& ${\surd}$ & ${\surd}$ & ${\surd}$ &${\surd}$&$100\%$ \\
pattern9-6 & Err(2)& ${\surd}$ & ${\surd}$ & Err(2) &Err(2)&$40\%$ \\
\hline
正確率& $83.3\%$ & $100\%$ &$83.3\%$ &$50\%$ & $67.7\%$ &$76.7\%$\\
\hline
\end{tabular}
\end{table}

上記の表4によると、以下のことが分かった。
\begin{itemize}
\item[1] $K=3$の時識別率が一番良い、$K=7$ の時識別率が一番悪い。
\subitem(i)\small  必ずしも$K$が大きいほうが良い。$K$の選択法は評価結果にかなり影響を与える。
\item[2] \normalsize 各未知パターンに応じて、pattern2-5, pattern2-6, pattern9-5の識別率は相当に良いが、pattern7-5とpattern9-6の識別率が不十分である。誤識別率の高いパターンにおいて:
\subitem(i)\small  pattern7-5では、$K$近傍の中に7と9が出現率が一緒の場合が多くて、数字7か数字9か識別できなくなる。
\subitem(ii)\small 一方、pattern9-6では、数字2と誤識別される場合が多い。いわゆる、数字2と相似である。

\end{itemize}
実際K近傍法を応用する時、K値を決め方の一つは、K-fold Cross Validation(K-分割交差検証)という方法がよく使われている\cite{K-fold Cross Validation}。
\subsection{プログラムの互換性}
パターンデータの保存方法に関して、以下の構造体の利用した。
\begin{lstlisting}[language=c]
typedef struct {
  double **data; /*Character Datas (2 Demention Variable Length Array)*/
  int width; /*Width of Data matrix*/
  int height; /*Height of Data Matrix*/
  int pattern;/*Pattern type*/
} character_data;
\end{lstlisting}

構造体の中に、パターンである二値画像をデータの可変長配列、配列の横幅、縦幅、パターン種類で保存される。それぞれの要素を各学習ファイルから獲得できるような処理を行い、データ種類やデータ量が数多くなる時もう対応できるようになる。

さらに、パターンが数字でなく文字の場合、[pattern]の変数型を変更すれば良い、他の種類のパターンも対応できるようになる。
\subsection{プログラムの欠点}
gprofコマンドを用いてプログラムの関数呼び出し回数を解析した結果。プログラムの時間複雑度は$O(n^{2})$数量級である。

だが、今回の課題では、全ての学習データを一元構造体配列に格納するとこになったが、距離を計算した後どの学習パターンを値を得るなどの処理が重くなる。一つの提案として、全ての学習データを連結リストの構造で保存すれば、距離を参照し、相応しい学習パターンを得る処理がポインターで実現すれば、時間複雑度と後の拡張プログラム作業量も減らせると考えられる。

\section{パターン認識の使用例}
\subsection{音声認識の分野}
人間が話す音声が機械に認識される一連の処理には、自然音声の音声学特徴を抽出し、音声モデルに応用されるのは一般的である。
よく使われる特徴量は：
\begin{itemize}
\item[1] 線形予測係数
\item[2] ケプストラム
\item[3] MFCC(メル周波数ケプストラム係数)
\end{itemize}
である。
これらの特徴量を利用し、実際の文字配列と対応する音声モデルが生成可能になる\cite{音声認識}。

\begin{thebibliography}{9}
    \bibitem{K-fold Cross Validation} Refaeilzadeh P, Tang L, Liu H. Cross-validation[M]//Encyclopedia of database systems. Springer US, 2009: 532-538.
    \bibitem{音声認識}今井聖. (1995). 音声認識. 情報 電子入門シリーズ 16.
\end{thebibliography}

\end{document}